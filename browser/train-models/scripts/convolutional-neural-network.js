import * as tf from "@tensorflow/tfjs";
import * as tfvis from "@tensorflow/tfjs-vis";
import { saveModel, loadModel } from "../../helpers";
import { CnnData } from "./cnn-data";
const IMAGE_WIDTH = 28;
const IMAGE_HEIGHT = 28;
const IMAGE_CHANNELS = 1;
const STORAGE = "train-models/convoltional-neural-network";

export default class ConvoltionalNeuralNetwork {
  constructor() {
    this.classNames = [
      "Zero",
      "One",
      "Two",
      "Three",
      "Four",
      "Five",
      "Six",
      "Seven",
      "Eight",
      "Nine"
    ];
  }

  getModel() {
    this.model = tf.sequential();

    // In the first layer of our convolutional neural network we have
    // to specify the input shape. Then we specify some parameters for
    // the convolution operation that takes place in this layer.
    console.log(`CONV2D LAYER:
      inputShape: [width , height, depth],
      kernelSize: The size of the sliding convolutional filter windows to be applied to the input data - 5x5 in this case
      filters: The number of filter windows of size kernelSize to apply to the input data. Here, we will apply 8 filters to the data.
      strides: Step size - 1px in this case
      activation: The activation function to apply to the data after the convolution is complete. In this case, we are applying a Rectified Linear Unit (ReLU) function, which is a very common activation function in ML models.
      kernelInitializer: The method to use for randomly initializing the model weights, which is very important.
      `);
    this.model.add(
      tf.layers.conv2d({
        inputShape: [IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS],
        kernelSize: 5,
        filters: 8,
        strides: 1,
        activation: "relu",
        kernelInitializer: "varianceScaling"
      })
    );

    // The MaxPooling layer acts as a sort of downsampling using max values
    // in a region instead of averaging.
    this.model.add(
      tf.layers.maxPooling2d({ poolSize: [2, 2], strides: [2, 2] })
    );

    // Repeat another conv2d + maxPooling stack.
    // Note that we have more filters in the convolution.
    this.model.add(
      tf.layers.conv2d({
        kernelSize: 5,
        filters: 16,
        strides: 1,
        activation: "relu",
        kernelInitializer: "varianceScaling"
      })
    );
    this.model.add(
      tf.layers.maxPooling2d({ poolSize: [2, 2], strides: [2, 2] })
    );

    // Now we flatten the output from the 2D filters into a 1D vector to prepare
    // it for input into our last layer. This is common practice when feeding
    // higher dimensional data to a final classification output layer.
    console.log(`Flatten the output from the 2d filters into a 1d vector`);
    this.model.add(tf.layers.flatten());

    // Our last layer is a dense layer which has 10 output units, one for each
    // output class (i.e. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9).
    console.log(
      "The last layer is a dense layer that has x output units each for each output class, in this case 0 to 9"
    );
    const NUM_OUTPUT_CLASSES = 10;
    this.model.add(
      tf.layers.dense({
        units: NUM_OUTPUT_CLASSES,
        kernelInitializer: "varianceScaling",
        activation: "softmax"
      })
    );
    console.log(
      "Remember that we wanted a one-to-ten mapping (one input image to ten probabilities). This is why we have 10 units in our output layer."
    );
    console.log(
      "As the name implies this is used when the output of our model is a probability distribution. categoricalCrossentropy measures the error between the probability distribution generated by the last layer of our model and the probability distribution given by our true label."
    );

    // Choose an optimizer, loss function and accuracy metric,
    // then compile and return the model
    const optimizer = tf.train.adam();
    this.model.compile({
      optimizer: optimizer,
      loss: "categoricalCrossentropy",
      metrics: ["accuracy"]
    });

    console.log("model", this.model);
    return this.model;
  }

  doPrediction(testDataSize = 500) {
    const testData = this.data.nextTestBatch(testDataSize);

    const testxs = testData.xs.reshape([
      testDataSize,
      IMAGE_WIDTH,
      IMAGE_HEIGHT,
      1
    ]);
    const labels = testData.labels.argMax([-1]);
    const preds = this.model.predict(testxs).argMax([-1]);

    console.log({ testData, preds, labels }, testData.labels.argMax([-1]));
    testxs.dispose();
    return [preds, labels];
  }

  async loadImage(url) {
    // Make a request for the MNIST sprited image.
    const img = new Image();
    const canvas = document.createElement("canvas");
    const ctx = canvas.getContext("2d");
    return new Promise((resolve, reject) => {
      img.crossOrigin = "*";
      img.onload = () => {
        img.width = img.naturalWidth;
        img.height = img.naturalHeight;

        ctx.drawImage(img, 0, 0);
        const tensor = tf.browser.fromPixels(canvas);

        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
        resolve({
          url,
          imageData,
          tensor
        });
      };
      img.src = url;
    });
  }

  async predictManualy() {
    const images = {
      8: "http://localhost:314/browser/train-models/images/8.png",
      3: "http://localhost:314/browser/train-models/images/3.png"
    };
    const inputs = [];
    const labels = [];

    // Await load images
    let keys = Object.keys(images);
    for (let i = 0; i < keys.length; i++) {
      const key = keys[i];
      const input = await this.loadImage(images[key]);
      const label = key;
      console.log({ input, label });

      labels.push(label);
      inputs.push(input.tensor);
    }

    const tensorInputs = inputs;
    const tensorLabels = labels.map(el => tf.tensor(el));
    const testData = {
      xs: tensorInputs,
      labels: tensorLabels
    };
    const testxs = testData.xs.reshape([2, IMAGE_WIDTH, IMAGE_HEIGHT, 1]);

    const _labels = testData.labels.argMax([-1]);
    const _preds = this.model.predict(testxs).argMax([-1]);

    console.log({ _labels, _preds });
    testxs.dispose();
    /*
    const testData = this.data.nextTestBatch(testDataSize);

    const testxs = testData.xs.reshape([
      testDataSize,
      IMAGE_WIDTH,
      IMAGE_HEIGHT,
      1
    ]);
    const labels = testData.labels.argMax([-1]);
    const preds = this.model.predict(testxs).argMax([-1]);

    console.log({ testData, preds, labels }, testData.labels.argMax([-1]));
    testxs.dispose();
    return [preds, labels];
    */
    return {
      images,
      tensorInputs,
      tensorLabels
    };
  }

  async showAccuracy() {
    const [preds, labels] = this.doPrediction();
    const classAccuracy = await tfvis.metrics.perClassAccuracy(labels, preds);
    const container = { name: "Accuracy", tab: "Evaluation" };
    tfvis.show.perClassAccuracy(container, classAccuracy, this.classNames);
    console.log({ preds, labels }, "do accuracy");
    labels.dispose();
  }

  async showConfusion() {
    const [preds, labels] = this.doPrediction();
    const confusionMatrix = await tfvis.metrics.confusionMatrix(labels, preds);
    const container = { name: "Confusion Matrix", tab: "Evaluation" };
    tfvis.render.confusionMatrix(
      container,
      { values: confusionMatrix },
      this.classNames
    );

    labels.dispose();
  }

  async showExamples() {
    // Create a container in the visor
    const surface = tfvis
      .visor()
      .surface({ name: "Input Data Examples", tab: "Input Data" });

    // Get the examples
    const examples = this.data.nextTestBatch(20);
    const numExamples = examples.xs.shape[0];

    // Create a canvas element to render each example
    for (let i = 0; i < numExamples; i++) {
      const imageTensor = tf.tidy(() => {
        // Reshape the image to 28x28 px
        return examples.xs
          .slice([i, 0], [1, examples.xs.shape[1]])
          .reshape([28, 28, 1]);
      });

      const canvas = document.createElement("canvas");
      canvas.width = 28;
      canvas.height = 28;
      canvas.style = "margin: 4px;";
      await tf.browser.toPixels(imageTensor, canvas);
      surface.drawArea.appendChild(canvas);

      imageTensor.dispose();
    }
  }

  async train() {
    const metrics = ["loss", "val_loss", "acc", "val_acc"];
    const container = {
      name: "Model Training",
      styles: { height: "1000px" }
    };
    const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);

    const BATCH_SIZE = 512;
    const TRAIN_DATA_SIZE = 5500;
    const TEST_DATA_SIZE = 1000;

    const [trainXs, trainYs] = tf.tidy(() => {
      const d = this.data.nextTrainBatch(TRAIN_DATA_SIZE);
      return [d.xs.reshape([TRAIN_DATA_SIZE, 28, 28, 1]), d.labels];
    });

    const [testXs, testYs] = tf.tidy(() => {
      const d = this.data.nextTestBatch(TEST_DATA_SIZE);
      return [d.xs.reshape([TEST_DATA_SIZE, 28, 28, 1]), d.labels];
    });

    return this.model.fit(trainXs, trainYs, {
      batchSize: BATCH_SIZE,
      validationData: [testXs, testYs],
      epochs: 10,
      shuffle: true,
      callbacks: fitCallbacks
    });
  }

  async run() {
    console.log(`
      DATA:
      * Load and parse stripe file:
        * Breaks the sprite in small squares
        * Maps the MNIST labels to the small squares
        * Handles the loaded data as small batches to handle better the memory
      CNN:
      * Load data
      * Show Examples - 20 samples of 28*28 pixels and 1 color channel
      * Get The Model - In machine learning we define an architecture (or algorithm) and let the training process learn the parameters of that algorithm.
      * Train the model:
        * Define the metrics we are going to monitor ['loss', 'val_loss', 'acc', 'val_acc']
        * Prepare data as tensors (2 datasets, 1 for training and 1 for validation)

    `);
    this.data = new Data();
    await this.data.load();

    await this.showExamples(this.data);
    console.log(`
      Our goal is to train a model that will take one image and learn to predict a score for each of the possible 10 classes that image may belong to (the digits 0-9).
      Each image is 28px wide 28px high and has a 1 color channel as it is a grayscale image. So the shape of each image is [28, 28, 1].
      Remember that we do a one-to-ten mapping, as well as the shape of each input example, since it is important for the next section.`);

    this.getModel();
    tfvis.show.modelSummary({ name: "Model Architecture" }, this.model);

    const _model = await loadModel(STORAGE);
    // console.log({ _model }, "XXXXXXXXXXXXXXXX");
    // console.log("Start training");

    await await this.train();
    // await saveModel(this.model, STORAGE);
    await this.showAccuracy();
    await this.showConfusion();

    const manualPrediction = await this.predictManualy();
    console.log({ manualPrediction });
  }
}
